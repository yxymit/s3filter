- create an EC2 Ubuntu instance on AWS (MainServerSetup)
- use IMDSv1 for meta data service
- attach IAM role (EC2_role) to the instance for access to lambda and S3
- WARN: stop instance rather than terminate, unless you know what you are doing


- setup conda for package management
# mkdir -p ~/miniconda3_1
# wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3_1/miniconda.sh
# bash ~/miniconda3_1/miniconda.sh -b -u -p ~/miniconda3_1
# rm -rf ~/miniconda3_1/miniconda.sh

- env (not include python-minimal python-pip)
# sudo apt -y update
# sudo apt -y install graphviz pkg-config awscli libgraphviz-dev graphviz-dev python3-dev gcc
# export PYTHONPATH=/home/ubuntu/s3filter_cloud

- repo
# git clone https://github.com/LongEarW/s3filter_cloud.git
- packages 
# ~/miniconda3_1/bin/conda install -c conda-forge python-primesieve
# ~/miniconda3_1/bin/pip3 install -r requirements.txt
# export PATH="/home/ubuntu/miniconda3_1/bin/python3:$PATH"

- Prepare data
- downlaod tool from TPC (https://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp) and generate data
  or TPC for MACOS (https://github.com/gregrahn/tpch-kit/tree/master)
# cd tpch-kit
# rm -r ref_data
# mkdir ref_data
# make MACHINE=MACOS DATABASE=POSTGRESQL
# export DSS_CONFIG=/.../tpch-kit/dbgen
# export DSS_QUERY=$DSS_CONFIG/queries
# export DSS_PATH=/.../tpch-kit/ref_data
# ./dbgen -vf -s 1 -TL -C 10 -S 1
# ./dbgen -vf -s 1 -TL -C 10 -S 2 
# ...

- convert TPC-H dataset to CSV
# python3 tbl2csv.py

- S3
- create S3 bucket (s3filter-289785222077)
- index
# access_method_benchmark/shards-1GB/index/index.lineitem.{}.csv
- data
# access_method_benchmark/shards-1GB/lineitem.{}.csv

- Lambda
- Create IAM with access of S3
- Create Lambda fucntion
- Add Pandas package in function layor (arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python38:11)
- Attach IAM as excetion role